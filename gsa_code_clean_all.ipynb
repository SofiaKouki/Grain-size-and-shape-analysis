{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Luhfl9dJUZfg"
      },
      "outputs": [],
      "source": [
        "# This model is designed to perform automated segmentation and classification of sediment features.\n",
        "# It focuses on identifying specific particle properties and categorizing them into distinct classes.\n",
        "# The pipeline starts by using a U-Net-based convolutional neural network to segment images, separating grains from the surrounding matrix.\n",
        "# Once the grains are segmented, the model extracts a range of geometric and shape-related properties for each particle.\n",
        "# These properties include roundness, elongation, and rugosity, as well as measurements like perimeter, Feretâ€™s diameter, and overall particle size.\n",
        "# The classification stage employs several machine learning algorithms, including support vector machines (SVM), logistic regression, and Naive Bayes.\n",
        "# The model is trained on a balanced dataset that ensures equitable representation across different particle categories (for roundness, elongation, and rugosity).\n",
        "\n",
        "\n",
        "############################### Import the essential libraries ######################\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import measure\n",
        "from skimage.io import imread\n",
        "from skimage.measure import regionprops, label\n",
        "from scipy.stats import skew, kurtosis\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import resample\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
        "\n",
        "\n",
        "############## Constants ##########################\n",
        "IMAGE_DIR = r'C:\\Users\\Documents\\gsd'\n",
        "SAVE_DIRECTORY = r'C:\\Users\\Documents'\n",
        "SEGMENTED_IMAGE_DIR = r'C:\\Users\\Documents\\grain_masks_processed_1'\n",
        "\n",
        "PIXELS_PER_MM = 900\n",
        "CALIBRATION_FACTOR = 1 / PIXELS_PER_MM\n",
        "features = ['Roundness', 'Elongation', 'Rugosity']\n",
        "\n",
        "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 1024, 1024, 3\n",
        "\n",
        "############## Data Loading with Preprocessing ##############\n",
        "def load_data(image_path):\n",
        "    images = []\n",
        "    masks = []\n",
        "    for file in os.listdir(image_path):\n",
        "        if file.endswith('.tif') or file.endswith('.png'):\n",
        "            img = cv2.imread(os.path.join(image_path, file), cv2.IMREAD_COLOR)\n",
        "            if img is None:\n",
        "                print(f\"Error loading image: {file}\")\n",
        "                continue\n",
        "\n",
        "            # Denoise image if needed\n",
        "            img = cv2.GaussianBlur(img, (0, 0), sigmaX=2, sigmaY=2)\n",
        "\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT)) / 255.0\n",
        "            images.append(img)\n",
        "\n",
        "            gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
        "            _, mask = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
        "            mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT)) / 255.0\n",
        "            masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Use IMAGE_DIR instead of undefined data_path\n",
        "images, masks = load_data(IMAGE_DIR)\n",
        "print(f\"Loaded {len(images)} images and {len(masks)} masks\")\n",
        "\n",
        "if len(images) == 0 or len(masks) == 0:\n",
        "    raise ValueError(\"No images or masks loaded. Check the input directory and image files.\")\n",
        "\n",
        "masks = np.expand_dims(masks, axis=-1)\n",
        "\n",
        "############## Split Data ##############\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
        "\n",
        "############## Build U-Net segmentation model ################\n",
        "def build_unet():\n",
        "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    u6 = UpSampling2D((2, 2))(c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u7 = UpSampling2D((2, 2))(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    u8 = UpSampling2D((2, 2))(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "\n",
        "    u9 = UpSampling2D((2, 2))(c8)\n",
        "    u9 = concatenate([u9, c1])\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "model = build_unet()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=4,\n",
        "    epochs=10)\n",
        "\n",
        "preds = model.predict(X_val)\n",
        "\n",
        "\n",
        "############################# Feature extraction and grain size and shape properties measurement ##################@###\n",
        "\n",
        "def extract_particle_properties(image_path, calibration_factor):\n",
        "    image = imread(image_path, as_gray=True)\n",
        "    binary_image = image > 0.5\n",
        "    cleared_particles = label(binary_image)\n",
        "    image_area_mm2 = image.shape[0] * image.shape[1] * (calibration_factor ** 2)\n",
        "    return cleared_particles, image_area_mm2\n",
        "\n",
        "def classify_sorting(coefficient_of_variation):\n",
        "    if coefficient_of_variation < 0.35:\n",
        "        return 'Very Well Sorted'\n",
        "    elif coefficient_of_variation < 0.5:\n",
        "        return 'Well Sorted'\n",
        "    elif coefficient_of_variation < 0.71:\n",
        "        return 'Moderately Sorted'\n",
        "    elif coefficient_of_variation < 2.0:\n",
        "        return 'Poorly Sorted'\n",
        "    else:\n",
        "        return 'Very Poorly Sorted'\n",
        "\n",
        "def process_directory(directory, save_dir, calibration_factor):\n",
        "    all_data = {}\n",
        "    summary_data = []\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(('.tif', '.jpg', '.png')):\n",
        "            image_path = os.path.join(directory, filename)\n",
        "            cleared_particles, total_image_area_mm2 = extract_particle_properties(image_path, calibration_factor)\n",
        "            props = regionprops(cleared_particles)\n",
        "\n",
        "########### Collect properties for each particle ##########################\n",
        "            particle_data = {\n",
        "                \"Label\": [],\n",
        "                \"Perimeter (mm)\": [],\n",
        "                \"Feret's Diameter (mm)\": [],\n",
        "                \"Roundness\": [],\n",
        "                \"Rugosity\": [],\n",
        "                \"Elongation\": [],\n",
        "                \"Particle Size (mm)\": []\n",
        "            }\n",
        "\n",
        "            for prop in props:\n",
        "                particle_data[\"Label\"].append(prop.label)\n",
        "                particle_data[\"Perimeter (mm)\"].append(prop.perimeter * calibration_factor)\n",
        "                particle_data[\"Feret's Diameter (mm)\"].append(prop.equivalent_diameter * calibration_factor)\n",
        "                particle_data[\"Roundness\"].append(\n",
        "                    4 * np.pi * (prop.area * calibration_factor ** 2) / (prop.perimeter * calibration_factor) ** 2\n",
        "                )\n",
        "                particle_data[\"Rugosity\"].append(\n",
        "                    2 * np.sqrt(np.pi * (prop.area * calibration_factor ** 2)) / (prop.perimeter * calibration_factor)\n",
        "                )\n",
        "                particle_data[\"Elongation\"].append(prop.major_axis_length / prop.minor_axis_length)\n",
        "                particle_data[\"Particle Size (mm)\"].append(np.sqrt(prop.area * calibration_factor ** 2))\n",
        "\n",
        "            sheet_name = os.path.splitext(filename)[0]\n",
        "            df = pd.DataFrame(particle_data)\n",
        "            all_data[sheet_name] = df\n",
        "\n",
        "\n",
        "##################### Summary statistics ########################\n",
        "            total_particle_area_mm2 = sum([prop.area * (calibration_factor ** 2) for prop in props])\n",
        "            percentage_area_occupied = (total_particle_area_mm2 / total_image_area_mm2) * 100\n",
        "\n",
        "            particle_size_mm_series = pd.Series(particle_data[\"Particle Size (mm)\"]).dropna()\n",
        "            coefficient_of_variation = (\n",
        "                particle_size_mm_series.std() / particle_size_mm_series.mean()\n",
        "                if not particle_size_mm_series.empty and particle_size_mm_series.mean() != 0 else 0\n",
        "            )\n",
        "\n",
        "            summary_data.append({\n",
        "                \"Image Name\": sheet_name,\n",
        "                \"Number of Particles\": len(props),\n",
        "                \"Percentage Area Occupied\": f\"{percentage_area_occupied:.2f}%\",\n",
        "                \"Mean Particle Size (mm)\": particle_size_mm_series.mean() if not particle_size_mm_series.empty else 0,\n",
        "                \"Coefficient of Variation\": coefficient_of_variation,\n",
        "                \"Degree of Sorting\": classify_sorting(coefficient_of_variation)\n",
        "            })\n",
        "\n",
        "    save_path = os.path.join(save_dir, 'particle_properties_all_images.xlsx')\n",
        "    with pd.ExcelWriter(save_path, engine='openpyxl') as writer:\n",
        "        for sheet_name, df in all_data.items():\n",
        "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "        pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
        "    print(\"Particle properties saved to:\", save_path)\n",
        "    return save_path\n",
        "\n",
        "#################### Classify the grains based on the degree of roundness, rugosity, and elongation ##############################\n",
        "def apply_classifications(df):\n",
        "\n",
        "    def classify_roundness(val):\n",
        "        if val < 0.25:\n",
        "            return 'Angular'\n",
        "        elif 0.25 <= val < 0.5:\n",
        "            return 'Sub-angular'\n",
        "        elif 0.5 <= val < 0.75:\n",
        "            return 'Sub-rounded'\n",
        "        else:\n",
        "            return 'Rounded'\n",
        "\n",
        "    df['Roundness Category'] = df['Roundness'].apply(classify_roundness)\n",
        "\n",
        "    def classify_rugosity(val):\n",
        "        if val < 0.25:\n",
        "            return 'Low'\n",
        "        elif 0.25 <= val < 0.5:\n",
        "            return 'Moderate'\n",
        "        elif 0.5 <= val < 0.75:\n",
        "            return 'High'\n",
        "        else:\n",
        "            return 'Very High'\n",
        "\n",
        "    df['Rugosity Category'] = df['Rugosity'].apply(classify_rugosity)\n",
        "\n",
        "    def classify_elongation(val):\n",
        "        if val < 1.5:\n",
        "            return 'Equant'\n",
        "        elif 1.5 <= val < 2.5:\n",
        "            return 'Slightly Elongated'\n",
        "        elif 2.5 <= val < 3.5:\n",
        "            return 'Moderately Elongated'\n",
        "        else:\n",
        "            return 'Elongated'\n",
        "\n",
        "    df['Elongation Category'] = df['Elongation'].apply(classify_elongation)\n",
        "\n",
        "    return df\n",
        "\n",
        "def classify_and_save(input_file_path, output_file_path):\n",
        "    xls = pd.ExcelFile(input_file_path)\n",
        "    all_sheets_dict = {}\n",
        "\n",
        "    for sheet_name in xls.sheet_names:\n",
        "        df = pd.read_excel(xls, sheet_name=sheet_name)\n",
        "        all_sheets_dict[sheet_name] = apply_classifications(df)\n",
        "\n",
        "    with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:\n",
        "        for sheet_name, df in all_sheets_dict.items():\n",
        "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "    print(\"Classifications applied and saved to:\", output_file_path)\n",
        "    return output_file_path\n",
        "\n",
        "def analyze_and_balance(file_path):\n",
        "    excel_data = pd.read_excel(file_path, sheet_name=None)\n",
        "    combined_data = pd.concat(excel_data.values(), ignore_index=True)\n",
        "\n",
        "    categories = [\"Roundness Category\", \"Rugosity Category\", \"Elongation Category\"]\n",
        "    print(\"Class Distributions for All Sheets Combined:\")\n",
        "    for category in categories:\n",
        "        if category in combined_data.columns:\n",
        "            print(f\"\\nClass distribution for {category}:\")\n",
        "            print(combined_data[category].value_counts())\n",
        "        else:\n",
        "            print(f\"\\n{category} is not present in the data.\")\n",
        "\n",
        "    def balance_category(data, category, features):\n",
        "        print(f\"\\nBalancing the dataset for {category}...\")\n",
        "        target = data[category]\n",
        "        X = data[features]\n",
        "        y = target\n",
        "\n",
        "################# Discard classes with only one sample and apply SMOTE (use of 500 grains per class) ####################\n",
        "        class_counts = y.value_counts()\n",
        "        classes_to_keep = class_counts[class_counts > 1].index\n",
        "        filtered_data = data[data[category].isin(classes_to_keep)]\n",
        "        X = filtered_data[features]\n",
        "        y = filtered_data[category]\n",
        "\n",
        "        print(f\"Classes with >1 sample for {category}: {classes_to_keep.tolist()}\")\n",
        "        print(\"Original class distribution:\")\n",
        "        print(y.value_counts())\n",
        "\n",
        "        smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
        "        X_smote, y_smote = smote.fit_resample(X, y)\n",
        "        smote_data = pd.DataFrame(X_smote, columns=features)\n",
        "        smote_data[category] = y_smote\n",
        "\n",
        "        balanced_data = pd.DataFrame(columns=smote_data.columns)\n",
        "        for class_label in smote_data[category].unique():\n",
        "            class_subset = smote_data[smote_data[category] == class_label]\n",
        "            if len(class_subset) > 500:\n",
        "                class_subset = resample(class_subset, replace=False, n_samples=500, random_state=42)\n",
        "            balanced_data = pd.concat([balanced_data, class_subset])\n",
        "\n",
        "        print(\"Balanced class distribution:\")\n",
        "        print(balanced_data[category].value_counts())\n",
        "        return balanced_data\n",
        "\n",
        "    balanced_roundness = balance_category(combined_data, 'Roundness Category', features)\n",
        "    balanced_rugosity = balance_category(combined_data, 'Rugosity Category', features)\n",
        "    balanced_elongation = balance_category(combined_data, 'Elongation Category', features)\n",
        "\n",
        "    return balanced_roundness, balanced_rugosity, balanced_elongation\n",
        "\n",
        "\n",
        "########### Create the confusion matrix  and evaluate the models ##################\n",
        "def save_confusion_matrix(cm, classes, model_name, category_name, save_dir):\n",
        "    wrapped_classes = [\"\\n\".join(textwrap.wrap(str(label), 10)) for label in classes]\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=wrapped_classes, yticklabels=wrapped_classes, cbar=False,\n",
        "                annot_kws={\"size\": 12, \"weight\": \"bold\"})\n",
        "    plt.title(f'{model_name}\\nConfusion Matrix for {category_name}', fontsize=14, weight='bold')\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "    plt.yticks(rotation=0, fontsize=10)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    save_path = os.path.join(save_dir, f'{model_name}_{category_name}_confusion_matrix.png')\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Confusion matrix saved to: {save_path}\")\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(X, y, model, model_name, classes, category_name, save_dir):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"\\n{model_name} Performance for {category_name}:\")\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    save_confusion_matrix(cm, classes, model_name, category_name, save_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    properties_file = process_directory(SEGMENTED_IMAGE_DIR, SAVE_DIRECTORY, CALIBRATION_FACTOR)\n",
        "    classified_file = os.path.join(SAVE_DIRECTORY, 'particle_properties_all_images_classified.xlsx')\n",
        "    classify_and_save(properties_file, classified_file)\n",
        "    balanced_roundness, balanced_rugosity, balanced_elongation = analyze_and_balance(classified_file)\n",
        "\n",
        "    X_roundness = balanced_roundness[features]\n",
        "    y_roundness = balanced_roundness['Roundness Category']\n",
        "\n",
        "    X_rugosity = balanced_rugosity[features]\n",
        "    y_rugosity = balanced_rugosity['Rugosity Category']\n",
        "\n",
        "    X_elongation = balanced_elongation[features]\n",
        "    y_elongation = balanced_elongation['Elongation Category']\n",
        "\n",
        "######################## Define the models (SVM, LogRes, and Naive Bayes) ####################\n",
        "    models = [\n",
        "        (SVC(kernel='rbf', random_state=42, C=1.0, probability=True), \"Support Vector Machine\"),\n",
        "        (LogisticRegression(max_iter=50, random_state=42, penalty='l2', C=1.0), \"Logistic Regression\"),\n",
        "        (GaussianNB(), \"Naive Bayes\")\n",
        "    ]\n",
        "\n",
        "    # Get sorted class labels.\n",
        "    roundness_classes = sorted(balanced_roundness['Roundness Category'].unique())\n",
        "    rugosity_classes = sorted(balanced_rugosity['Rugosity Category'].unique())\n",
        "    elongation_classes = sorted(balanced_elongation['Elongation Category'].unique())\n",
        "    evaluation_dir = r'C:\\Users\\skkou\\Documents\\evaluation'\n",
        "    for model_obj, model_name in models:\n",
        "        print(\"\\nClassifying Roundness Category\")\n",
        "        train_and_evaluate_model(X_roundness, y_roundness, model_obj, model_name, roundness_classes, 'Roundness', confusion_save_dir)\n",
        "\n",
        "        print(\"\\nClassifying Rugosity Category\")\n",
        "        train_and_evaluate_model(X_rugosity, y_rugosity, model_obj, model_name, rugosity_classes, 'Rugosity', confusion_save_dir)\n",
        "\n",
        "        print(\"\\nClassifying Elongation Category\")\n",
        "        train_and_evaluate_model(X_elongation, y_elongation, model_obj, model_name, elongation_classes, 'Elongation', confusion_save_dir)\n"
      ]
    }
  ]
}